import { loggers } from '@/utils/logger';
import { encode } from '@toon-format/toon';
import OpenAI from 'openai';
import type { AIProvider, AIResponse, Message } from './types';

/**
 * Provider unificado usando Cloudflare AI Gateway
 * Usa SDK OpenAI apontando para o AI Gateway compat endpoint
 * Ref: https://developers.cloudflare.com/ai-gateway/
 *
 * Benef√≠cios:
 * - Fallback autom√°tico via Dynamic Routes
 * - Cache, rate limiting, analytics nativos
 * - Retry autom√°tico (at√© 5 tentativas)
 * - Hist√≥rico convertido para TOON (economia de tokens)
 */
export class CloudflareAIGatewayProvider implements AIProvider {
	private client: OpenAI;
	private model: string;

	constructor(accountId: string, gatewayId: string, cfApiToken: string, model = 'dynamic/cloudflare') {
		const baseURL = `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/compat`;

		this.client = new OpenAI({
			apiKey: cfApiToken,
			baseURL,
		});
		this.model = model;

		loggers.ai.info(`‚úÖ AI Gateway configurado: ${baseURL} (model: ${model})`);
	}

	getName(): string {
		return 'ai-gateway';
	}

	setModel(model: string): void {
		this.model = model;
		loggers.ai.info(`üîÑ Model alterado para: ${model}`);
	}

	async callLLM(params: { message: string; history?: Message[]; systemPrompt?: string }): Promise<AIResponse> {
		const { message, history = [], systemPrompt } = params;
		const startTime = Date.now();

		try {
			loggers.ai.info(`üöÄ Enviando para AI Gateway (model: ${this.model})`);

			// Converter hist√≥rico para TOON (economiza 30-60% tokens)
			let contextContent = message;

			if (history.length > 0) {
				const historyData = history.map((msg) => ({
					role: msg.role,
					content: msg.content,
				}));

				const toonHistory = encode(historyData, { delimiter: '\t' });

				contextContent = `Hist√≥rico da conversa em formato TOON (tab-separated):

\`\`\`toon
${toonHistory}
\`\`\`

Mensagem atual: ${message}`;
			}

			// Montar messages no formato OpenAI
			const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [];

			if (systemPrompt) {
				messages.push({
					role: 'system',
					content: systemPrompt,
				});
			}

			messages.push({
				role: 'user',
				content: contextContent,
			});

			// Chamada ao AI Gateway - retry e fallback s√£o autom√°ticos
			const response = await this.client.chat.completions.create({
				model: this.model,
				messages,
			});

			const duration = Date.now() - startTime;
			const rawContent = response.choices[0]?.message?.content;

			// Log estruturado
			loggers.ai.info(
				{
					response: {
						id: response.id,
						model: response.model,
						duration,
						usage: response.usage,
					},
				},
				`‚ú® Resposta do AI Gateway em ${duration}ms`,
			);

			let text = '';
			if (typeof rawContent === 'string') {
				text = rawContent;
			} else if (typeof rawContent === 'object' && rawContent !== null) {
				text = JSON.stringify(rawContent);
			}

			if (!text || text.trim().length === 0) {
				loggers.ai.error({ rawContent }, '‚ö†Ô∏è Resposta vazia do AI Gateway');
				throw new Error('AI Gateway returned empty response');
			}

			return {
				message: text.trim(),
			};
		} catch (error: any) {
			const duration = Date.now() - startTime;
			loggers.ai.error(
				{
					err: error,
					duration,
					model: this.model,
				},
				'‚ùå Erro no AI Gateway',
			);
			throw error;
		}
	}
}
